auth_enabled: false

server:
  http_listen_port: 3100

common:
  path_prefix: /mnt/loki
  storage:
    filesystem:
      chunks_directory: /mnt/data/chunks
      rules_directory: /mnt/data/rules
  replication_factor: 1
  ring:
    instance_addr: 127.0.0.1
    kvstore:
      store: inmemory

schema_config:
  configs:
    - from: 2024-04-01
      store: tsdb
      object_store: filesystem
      schema: v13
      index:
        prefix: index_
        period: 24h

storage_config:
  filesystem:
    directory: /mnt/data/chunks

  tsdb_shipper:
    active_index_directory: /mnt/data/tsdb-index
    cache_location: /mnt/data/tsdb-cache

# https://grafana.com/docs/loki/latest/operations/storage/retention/
compactor:
  working_directory: /mnt/loki/compactor
  compaction_interval: 10m
  retention_enabled: true
  retention_delete_delay: 2h
  retention_delete_worker_count: 150
  delete_request_store: filesystem

limits_config:
  reject_old_samples: true
  reject_old_samples_max_age: 168h
  ingestion_rate_mb: 64
  ingestion_burst_size_mb: 64
  per_stream_rate_limit: 64MB
  per_stream_rate_limit_burst: 64MB
  # To avoid querying of data beyond the retention period (table manager)
  max_query_lookback: 4380h
  # 6 months log retention
  retention_period: 4380h

query_scheduler:
  # the TSDB index dispatches many more, but each individually smaller, requests.
  # We increase the pending request queue sizes to compensate.
  max_outstanding_requests_per_tenant: 32768

querier:
  # Each `querier` component process runs a number of parallel workers to process queries simultaneously.
  # You may want to adjust this up or down depending on your resource usage
  # (more available cpu and memory can tolerate higher values and vice versa),
  # but we find the most success running at around `16` with tsdb
  max_concurrent: 16

ruler:
  storage:
    type: local
    local:
      directory: /etc/loki/rules
  rule_path: /tmp/
  alertmanager_url: http://alertmanager:9093
  ring:
    kvstore:
      store: inmemory
  enable_api: true
  enable_alertmanager_v2: true